# ğŸ¤– test_any_policy

A ManiSkill-based project for visualizing any robotic policy or trajectory.

### ğŸ“¦ Installation

```bash
# Create conda environment
conda create -n mani python=3.12

# Activate environment
conda activate mani

# Install ManiSkill
pip install --upgrade mani_skill

# (Optional) Install Open3D
pip install open3d
```

## ğŸ“‚ Data Preparation

### ğŸ—„ï¸ Articulated Objects

Download articulated objects (cabinets, drawers, etc.) from the PartNet-Mobility dataset:

1. **Browse & Download**: Visit [SAPIEN PartNet-Mobility Browser](https://sapien.ucsd.edu/browse)
2. **Extract**: Place downloaded models in `dataset/partnet_mobility/`
3. **Structure**: Each model should be in its own folder with the model ID as the folder name

```
dataset/
â””â”€â”€ partnet_mobility/
    â”œâ”€â”€ 12536/          # Model ID
    â”‚   â”œâ”€â”€ mobility.urdf
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ 45623/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ ...
```

### ğŸ¨ Custom Mesh Objects

Place your custom 3D mesh files (.obj, .stl, etc.) in the `dataset/customize/` directory:

```
dataset/
â””â”€â”€ customize/
    â”œâ”€â”€ mug_obj/
    â”‚   â””â”€â”€ base.obj
    â”œâ”€â”€ bottle_obj/
    â”‚   â””â”€â”€ model.obj
    â””â”€â”€ your_object/
        â””â”€â”€ mesh.obj
```

### ğŸ›¤ï¸ Trajectory Files

Trajectory files (`trajectory.json`) contain the robot TCP (Tool Center Point) poses in **camera coordinate system**.

Convert to SAPIEN world coordinate here `src/trajectory_loader.py`

You can specify a custom trajectory with `--trajectory-path`.

## ğŸ“¸ Usage

### GraspVLA Policy (`run_graspvla.py`)

Execute manipulation tasks using the GraspVLA vision-language-action policy.

> âš ï¸ **Important**: Before running this script, you must launch the GraspVLA server separately:
> ```bash
> python others/GraspVLA/vla_network/scripts/serve.py \
>     --port 6666 \
>     --path <path_to_model>
> ```

#### ğŸ¯ Basic Examples

**Default object with instruction:**
```bash
python scripts/graspvla/run_graspvla.py --instruction "pick up the mug"
```

---

### GraspVLA with PickClutterYCB (`run_graspvla_ycb.py`)

Execute manipulation tasks using GraspVLA policy with ManiSkill's PickClutterYCB-v1 environment (default YCB objects).

> âš ï¸ **Important**: Before running, launch the GraspVLA server (same as above).

#### ğŸ¯ Basic Examples

**Default scene:**
```bash
python scripts/graspvla/run_graspvla_ycb.py --instruction "pick up the object" --seed 42
```

### Capture Objects (`capture_custom_objects.py`)

Capture multi-view images of objects without robot interaction.

#### ğŸ¯ Basic Examples

**Default object:**
```bash
python scripts/capture/capture_custom_objects.py
```

**Articulated object (cabinet, drawer, etc.):**
```bash
python scripts/capture/capture_custom_objects.py --use-articulation --articulation-id 12536
```

**Custom position & rotation:**
```bash
python scripts/capture/capture_custom_objects.py --object-position 0 0 0.15 --object-rotation 90 0 0
```

### Capture Trajectory (`capture_trajectory.py`)

Execute robot trajectories and capture the entire manipulation process.

#### ğŸ¯ Basic Examples

**Generate Trajectory**
```bash
python scripts/capture/convert_gripper_pose.py 
```

**Custom robot & object positions:**
```bash
python scripts/capture/capture_trajectory.py --use_articulation --articulation_id 12536 --robot_position -0.6 -0.8 0 --robot_rotation 0 0 90
```

**Static capture (no trajectory):**
```bash
python scripts/capture/capture_trajectory.py --execute-trajectory False
```

**With grasp & lift:**
```bash
python scripts/capture/capture_trajectory.py --do-grasp-and-lift
```


## ğŸ“ Output

Both scripts generate:

```
outputs/<timestamp>/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ step_000000/
â”‚   â”‚   â”œâ”€â”€ front.png          # RGB image
â”‚   â”‚   â”œâ”€â”€ front_depth.npy    # Depth data
â”‚   â”‚   â”œâ”€â”€ front_depth.png    # Depth visualization
â”‚   â”‚   â””â”€â”€ ... (other views)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ front.mp4
â”‚   â”œâ”€â”€ front_depth.mp4
â”‚   â””â”€â”€ ...
â””â”€â”€ camera_params/
    â”œâ”€â”€ camera_params.json     # Camera parameters
    â””â”€â”€ camera_params.npz
```

### ğŸ“· Camera Views

6 predefined views: `front`, `behind`, `top`, `left`, `right`, `diagonal`

## ğŸ’¡ Tips

- ğŸ“ **Units**: Positions in meters, rotations in degrees
- ğŸ¨ **Quality**: Use `--shader rt` for high-quality ray-traced rendering
- ğŸ¬ **Video**: Videos are generated automatically if `--save-video True`
